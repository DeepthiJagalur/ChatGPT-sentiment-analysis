{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41311e27",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error retrieving https://api.twitter.com/2/search/adaptive.json?include_profile_interstitial_type=1&include_blocking=1&include_blocked_by=1&include_followed_by=1&include_want_retweets=1&include_mute_edge=1&include_can_dm=1&include_can_media_tag=1&include_ext_has_nft_avatar=1&include_ext_is_blue_verified=1&include_ext_verified_type=1&skip_status=1&cards_platform=Web-12&include_cards=1&include_ext_alt_text=true&include_ext_limited_action_results=false&include_quote_count=true&include_reply_count=1&tweet_mode=extended&include_ext_collab_control=true&include_ext_views=true&include_entities=true&include_user_entities=true&include_ext_media_color=true&include_ext_media_availability=true&include_ext_sensitive_media_warning=true&include_ext_trusted_friends_metadata=true&send_error_codes=true&simple_quoted_tweet=true&q=ChatGPT+since%3A2022-12-01+until%3A2023-04-11+%2C+lang%3Aen&tweet_search_mode=live&count=20&query_source=spelling_expansion_revert_click&pc=1&spelling_corrections=1&include_ext_edit_control=true&ext=mediaStats%2ChighlightedLabel%2ChasNftAvatar%2CvoiceInfo%2Cenrichments%2CsuperFollowMetadata%2CunmentionInfo%2CeditControl%2Ccollab_control%2Cvibe: blocked (403)\n",
      "4 requests to https://api.twitter.com/2/search/adaptive.json?include_profile_interstitial_type=1&include_blocking=1&include_blocked_by=1&include_followed_by=1&include_want_retweets=1&include_mute_edge=1&include_can_dm=1&include_can_media_tag=1&include_ext_has_nft_avatar=1&include_ext_is_blue_verified=1&include_ext_verified_type=1&skip_status=1&cards_platform=Web-12&include_cards=1&include_ext_alt_text=true&include_ext_limited_action_results=false&include_quote_count=true&include_reply_count=1&tweet_mode=extended&include_ext_collab_control=true&include_ext_views=true&include_entities=true&include_user_entities=true&include_ext_media_color=true&include_ext_media_availability=true&include_ext_sensitive_media_warning=true&include_ext_trusted_friends_metadata=true&send_error_codes=true&simple_quoted_tweet=true&q=ChatGPT+since%3A2022-12-01+until%3A2023-04-11+%2C+lang%3Aen&tweet_search_mode=live&count=20&query_source=spelling_expansion_revert_click&pc=1&spelling_corrections=1&include_ext_edit_control=true&ext=mediaStats%2ChighlightedLabel%2ChasNftAvatar%2CvoiceInfo%2Cenrichments%2CsuperFollowMetadata%2CunmentionInfo%2CeditControl%2Ccollab_control%2Cvibe failed, giving up.\n",
      "Errors: blocked (403), blocked (403), blocked (403), blocked (403)\n"
     ]
    },
    {
     "ename": "ScraperException",
     "evalue": "4 requests to https://api.twitter.com/2/search/adaptive.json?include_profile_interstitial_type=1&include_blocking=1&include_blocked_by=1&include_followed_by=1&include_want_retweets=1&include_mute_edge=1&include_can_dm=1&include_can_media_tag=1&include_ext_has_nft_avatar=1&include_ext_is_blue_verified=1&include_ext_verified_type=1&skip_status=1&cards_platform=Web-12&include_cards=1&include_ext_alt_text=true&include_ext_limited_action_results=false&include_quote_count=true&include_reply_count=1&tweet_mode=extended&include_ext_collab_control=true&include_ext_views=true&include_entities=true&include_user_entities=true&include_ext_media_color=true&include_ext_media_availability=true&include_ext_sensitive_media_warning=true&include_ext_trusted_friends_metadata=true&send_error_codes=true&simple_quoted_tweet=true&q=ChatGPT+since%3A2022-12-01+until%3A2023-04-11+%2C+lang%3Aen&tweet_search_mode=live&count=20&query_source=spelling_expansion_revert_click&pc=1&spelling_corrections=1&include_ext_edit_control=true&ext=mediaStats%2ChighlightedLabel%2ChasNftAvatar%2CvoiceInfo%2Cenrichments%2CsuperFollowMetadata%2CunmentionInfo%2CeditControl%2Ccollab_control%2Cvibe failed, giving up.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mScraperException\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSome Modules are Missing \u001b[39m\u001b[38;5;124m\"\u001b[39m,e)\n\u001b[0;32m---> 12\u001b[0m \u001b[38;5;28;43;01mclass\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;21;43;01mtweetCollection\u001b[39;49;00m\u001b[43m(\u001b[49m\u001b[38;5;28;43mobject\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mChatGPT since:2022-12-01 until:2023-04-11 , lang:en\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlimit\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1000\u001b[39;49m\n",
      "Cell \u001b[0;32mIn[1], line 16\u001b[0m, in \u001b[0;36mtweetCollection\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m limit \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1000\u001b[39m\n\u001b[1;32m     15\u001b[0m tweets_1 \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m tweet \u001b[38;5;129;01min\u001b[39;00m sntwitter\u001b[38;5;241m.\u001b[39mTwitterSearchScraper(query)\u001b[38;5;241m.\u001b[39mget_items():\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(tweets_1) \u001b[38;5;241m==\u001b[39m limit:\n\u001b[1;32m     18\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/snscrape/modules/twitter.py:1661\u001b[0m, in \u001b[0;36mTwitterSearchScraper.get_items\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1658\u001b[0m params \u001b[38;5;241m=\u001b[39m paginationParams\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m   1659\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcursor\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m-> 1661\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iter_api_data(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://api.twitter.com/2/search/adaptive.json\u001b[39m\u001b[38;5;124m'\u001b[39m, _TwitterAPIType\u001b[38;5;241m.\u001b[39mV2, params, paginationParams, cursor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cursor):\n\u001b[1;32m   1662\u001b[0m \t\u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_v2_timeline_instructions_to_tweets_or_users(obj)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/snscrape/modules/twitter.py:761\u001b[0m, in \u001b[0;36m_TwitterAPIScraper._iter_api_data\u001b[0;34m(self, endpoint, apiType, params, paginationParams, cursor, direction)\u001b[0m\n\u001b[1;32m    759\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    760\u001b[0m \t_logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRetrieving scroll page \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcursor\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 761\u001b[0m \tobj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_api_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mapiType\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreqParams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    762\u001b[0m \t\u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m    764\u001b[0m \t\u001b[38;5;66;03m# No data format test, just a hard and loud crash if anything's wrong :-)\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/snscrape/modules/twitter.py:727\u001b[0m, in \u001b[0;36m_TwitterAPIScraper._get_api_data\u001b[0;34m(self, endpoint, apiType, params)\u001b[0m\n\u001b[1;32m    725\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m apiType \u001b[38;5;129;01mis\u001b[39;00m _TwitterAPIType\u001b[38;5;241m.\u001b[39mGRAPHQL:\n\u001b[1;32m    726\u001b[0m \tparams \u001b[38;5;241m=\u001b[39m urllib\u001b[38;5;241m.\u001b[39mparse\u001b[38;5;241m.\u001b[39murlencode({k: json\u001b[38;5;241m.\u001b[39mdumps(v, separators \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m'\u001b[39m)) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m params\u001b[38;5;241m.\u001b[39mitems()}, quote_via \u001b[38;5;241m=\u001b[39m urllib\u001b[38;5;241m.\u001b[39mparse\u001b[38;5;241m.\u001b[39mquote)\n\u001b[0;32m--> 727\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get\u001b[49m\u001b[43m(\u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apiHeaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponseOkCallback\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_api_response\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    728\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    729\u001b[0m \tobj \u001b[38;5;241m=\u001b[39m r\u001b[38;5;241m.\u001b[39mjson()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/snscrape/base.py:251\u001b[0m, in \u001b[0;36mScraper._get\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 251\u001b[0m \t\u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mGET\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/snscrape/base.py:247\u001b[0m, in \u001b[0;36mScraper._request\u001b[0;34m(self, method, url, params, data, headers, timeout, responseOkCallback, allowRedirects, proxies)\u001b[0m\n\u001b[1;32m    245\u001b[0m \t_logger\u001b[38;5;241m.\u001b[39mfatal(msg)\n\u001b[1;32m    246\u001b[0m \t_logger\u001b[38;5;241m.\u001b[39mfatal(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mErrors: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(errors)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 247\u001b[0m \t\u001b[38;5;28;01mraise\u001b[39;00m ScraperException(msg)\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mReached unreachable code\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mScraperException\u001b[0m: 4 requests to https://api.twitter.com/2/search/adaptive.json?include_profile_interstitial_type=1&include_blocking=1&include_blocked_by=1&include_followed_by=1&include_want_retweets=1&include_mute_edge=1&include_can_dm=1&include_can_media_tag=1&include_ext_has_nft_avatar=1&include_ext_is_blue_verified=1&include_ext_verified_type=1&skip_status=1&cards_platform=Web-12&include_cards=1&include_ext_alt_text=true&include_ext_limited_action_results=false&include_quote_count=true&include_reply_count=1&tweet_mode=extended&include_ext_collab_control=true&include_ext_views=true&include_entities=true&include_user_entities=true&include_ext_media_color=true&include_ext_media_availability=true&include_ext_sensitive_media_warning=true&include_ext_trusted_friends_metadata=true&send_error_codes=true&simple_quoted_tweet=true&q=ChatGPT+since%3A2022-12-01+until%3A2023-04-11+%2C+lang%3Aen&tweet_search_mode=live&count=20&query_source=spelling_expansion_revert_click&pc=1&spelling_corrections=1&include_ext_edit_control=true&ext=mediaStats%2ChighlightedLabel%2ChasNftAvatar%2CvoiceInfo%2Cenrichments%2CsuperFollowMetadata%2CunmentionInfo%2CeditControl%2Ccollab_control%2Cvibe failed, giving up."
     ]
    }
   ],
   "source": [
    "#Collect Data from snscrape to \n",
    "try:\n",
    "    import pymongo\n",
    "    from pymongo import MongoClient\n",
    "    import pandas as pd\n",
    "    import json\n",
    "    import snscrape.modules.twitter as sntwitter\n",
    "    import pandas as pd\n",
    "except Exception as e:\n",
    "    print(\"Some Modules are Missing \",e)\n",
    "\n",
    "class tweetCollection(object):\n",
    "    query = 'ChatGPT since:2022-12-01 until:2023-04-11 , lang:en'\n",
    "    limit = 1000\n",
    "    tweets_1 = []\n",
    "    for tweet in sntwitter.TwitterSearchScraper(query).get_items():\n",
    "        if len(tweets_1) == limit:\n",
    "            break\n",
    "        else:\n",
    "            tweets_1.append([tweet.id,\n",
    "                             tweet.user.username,\n",
    "                             tweet.user.verified,\n",
    "                             tweet.user.location,\n",
    "                             tweet.user.followersCount,\n",
    "                             tweet.rawContent,\n",
    "                             tweet.date,\n",
    "                             tweet.retweetedTweet,\n",
    "                             tweet.lang])\n",
    "    df_1 = pd.DataFrame(tweets_1, columns=['UserId',\n",
    "                                           'UserName',\n",
    "                                           'Verified',\n",
    "                                           'Location',\n",
    "                                           'Followers',\n",
    "                                           'Tweet',\n",
    "                                           'Date',\n",
    "                                           'Retweeted',\n",
    "                                           'Language'])\n",
    "    df_1.to_csv('Twitter_Data.csv', index=False)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    tweetCollection()\n",
    "    myclient = pymongo.MongoClient(\"mongodb://localhost:27017/\")\n",
    "    mydb = myclient[\"mydb2\"]\n",
    "    mycol = mydb[\"mycollection2\"]\n",
    "    df=pd.read_csv(\"Twitter_Data.csv\",low_memory=False)\n",
    "    data=df.to_dict(orient=\"records\")\n",
    "    mycol.insert_many(data)\n",
    "    print(\"Inserted Data\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ec85ad85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded\n"
     ]
    }
   ],
   "source": [
    "import pymongo\n",
    "import pandas as pd\n",
    "from pymongo import MongoClient\n",
    "#To get Data from MongoDB to csv file\n",
    "class MongoDB(object):\n",
    "\n",
    "    def __init__(self, dBName=None, collectionName=None):\n",
    "\n",
    "        self.dBName = dBName\n",
    "        self.collectionName = collectionName\n",
    "        self.client = MongoClient(\"localhost\", 27017, maxPoolSize=50)\n",
    "        self.DB = self.client[self.dBName]\n",
    "        self.collection = self.DB[self.collectionName]\n",
    "\n",
    "    def dbFetch(self, dBName=None, collectionName=None):\n",
    "        myclient = pymongo.MongoClient(\"mongodb://localhost:27017/\")\n",
    "        mydb = myclient[\"mydb\"]\n",
    "        mycol = mydb[\"mycollection\"]\n",
    "        x = mycol.find()\n",
    "        tweets_1 = []\n",
    "        for tweet in x:\n",
    "            \n",
    "            if 'Timestamp' in tweet:\n",
    "                tweets_1.append([tweet['UserId'],\n",
    "                                 tweet['UserName'],\n",
    "                                 tweet['Verified'],\n",
    "                                 tweet['Location'],\n",
    "                                 tweet['Followers'],\n",
    "                                 tweet['Tweet'],\n",
    "                                 tweet['Timestamp'],\n",
    "                                 tweet['Retweeted'],\n",
    "                                 tweet['Language']])\n",
    "            \n",
    "            \n",
    "            df_1 = pd.DataFrame(tweets_1, columns=['UserId',\n",
    "                                           'UserName',\n",
    "                                           'Verified',\n",
    "                                           'Location',\n",
    "                                           'Followers',\n",
    "                                           'Tweet',\n",
    "                                           'Timestamp',\n",
    "                                           'Retweeted',\n",
    "                                           'Language'])\n",
    "            df_1.to_csv('Twitter_Data.csv', index=False)\n",
    "            #print(tweet)\n",
    "        \n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    mongodb = MongoDB(dBName = 'mydb', collectionName='mycollection')\n",
    "    mongodb.dbFetch()\n",
    "    print(\"Downloaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adbd2b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import pymongo\n",
    "    from pymongo import MongoClient\n",
    "    import pandas as pd\n",
    "    import json\n",
    "    import snscrape.modules.twitter as sntwitter\n",
    "    import pandas as pd\n",
    "except Exception as e:\n",
    "    print(\"Some Modules are Missing \",e)\n",
    "    \n",
    "#Read data from csv stored from Mongodb\n",
    "df_tweets = pd.read_csv('Twitter_Data.csv')\n",
    "df_tweets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb78f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets = df_tweets.drop_duplicates()\n",
    "df_tweets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20dff2dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c22e06ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Divide the datetime columns into two separate columns\n",
    "df_tweets['Timestamp']=pd.to_datetime(df_tweets['Date'])\n",
    "df_tweets['Date'] = df_tweets['Timestamp'].dt.date\n",
    "df_tweets['Time_of_the_day'] = df_tweets['Timestamp'].dt.time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f47368",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets[df_tweets['Timestamp'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ffd293",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove missing values\n",
    "df_tweets.dropna(subset=['Timestamp'], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39795636",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save data for EDA \n",
    "df_tweets.to_csv('Twitter_EDA.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b19e6c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import re\n",
    "    import nltk\n",
    "    from nltk.corpus import stopwords\n",
    "    from nltk.stem import WordNetLemmatizer\n",
    "except Exception as e:\n",
    "    print(\"Some Modules are Missing \",e)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe212860",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_en = df_tweets[df_tweets['Language']=='en'].reset_index(drop=True)\n",
    "df_en.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a1986a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_en = df_en.drop(columns = ['Language'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68dd20bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove URL from tweet text\n",
    "df_en['Tweet'] = df_en['Tweet'].apply(lambda x: re.sub(r'http\\S+', '',x))\n",
    "#Remove mention (@user)\n",
    "df_en['Tweet'] = df_en['Tweet'].apply(lambda x: re.sub(r'@\\w+', '',x))\n",
    "#All lowercases\n",
    "df_en['Tweet'] = df_en['Tweet'].apply(lambda x: ' '.join(x.lower() for x in x.split()))\n",
    "#Remove Punctuation\n",
    "df_en['Tweet_punc'] = df_en['Tweet'].apply(lambda x: re.sub('[^\\w\\s]', '', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0110ef3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replace 'chat gpt' with 'chatgpt'\n",
    "df_en['Tweet_punc'] = df_en['Tweet_punc'].apply(lambda x: re.sub(r'chat gpt', 'chatgpt', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886e1bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import ssl\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae277705",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = stopwords.words('english')\n",
    "df_en['Tweet_stop'] = df_en['Tweet_punc'].apply(lambda x: ' '.join(x for x in x.split() if x not in stop))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0bb0b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_en['Tweet_tokenized'] = df_en['Tweet_stop'].apply(lambda x: re.split('\\W+', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d3a3b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d686cb38",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import pos_tag\n",
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7382ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define a function to map POS tags from treebank tag into wordnet tags\n",
    "wnl = WordNetLemmatizer()\n",
    "def get_wordnet(pos):\n",
    "    if pos.startswith('N'): \n",
    "        return wordnet.NOUN\n",
    "    elif pos.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif pos.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif pos.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN     #default to noun if no match\n",
    "\n",
    "    \n",
    "# Define a function to tag and lemmatize a text string    \n",
    "def lemmatizer(text):\n",
    "    pos_tags = pos_tag(text)\n",
    "    text = [wnl.lemmatize(word, pos = get_wordnet(pos)) for word, pos in pos_tags]\n",
    "    return text\n",
    "\n",
    "df_en['Tweet_lemmatized'] = df_en['Tweet_tokenized'].apply(lambda x: lemmatizer(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e2dea6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install spacy\n",
    "#!python -m spacy download en_core_web_sm\n",
    "#!pip install --upgrade dataclasses\n",
    "#import spacy\n",
    "# try spacy lemmatizer\n",
    "###\n",
    "def spacy_lemmatizer(text):\n",
    "    # only need tagger, no need for parser and named entity recognizer, for faster implementation\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    allowed_tags=['NOUN', 'ADJ', 'VERB', 'ADV']\n",
    "    doc = nlp(' '.join(text))\n",
    "    lemmas = [token.lemma_ for token in doc]\n",
    "    return lemmas\n",
    "#df_en['Tweet_lemmatized_spacy'] = df_en['Tweet_tokenized'].apply(lambda x: spacy_lemmatizer(x))\n",
    "###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064453ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a2467e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove columns not needed.\n",
    "df_en_clean = df_en.drop(columns=['Tweet', 'Tweet_punc', 'Tweet_stop', 'Tweet_tokenized'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87cb9678",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_en_clean['Tokens'] = df_en_clean['Tweet_lemmatized'].apply(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "023c5a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_en_clean.to_csv('Twitter_eng_clean.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b597d289",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a18d2e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
